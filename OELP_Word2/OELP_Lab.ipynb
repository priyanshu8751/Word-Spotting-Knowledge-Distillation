{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"mount_file_id":"1hGCduS6obw5vMGKHIx3_DR2drpJOFvnp","authorship_tag":"ABX9TyOSrEQFIjsYUM5nlYEJ5+f+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qf2i7esBULYb","executionInfo":{"status":"ok","timestamp":1664363035143,"user_tz":-330,"elapsed":5637,"user":{"displayName":"Priyanshu Gupta","userId":"04165208456498233301"}},"outputId":"271f4815-a049-429c-eb02-333005a6c04a"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import sys\n","import os\n","sys.path.append(os.path.abspath(\"/content/drive/MyDrive/OELP_Word \"))"],"metadata":{"id":"R__75PODVmGZ","executionInfo":{"status":"ok","timestamp":1664363037518,"user_tz":-330,"elapsed":4,"user":{"displayName":"Priyanshu Gupta","userId":"04165208456498233301"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":385},"id":"prMSjaOfT3bv","executionInfo":{"status":"error","timestamp":1664363465087,"user_tz":-330,"elapsed":540,"user":{"displayName":"Priyanshu Gupta","userId":"04165208456498233301"}},"outputId":"fd5db037-9f6d-4d4e-ec17-e428f2a3aa36"},"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(batch_size=16, decoder='bestpath', dump=False, early_stopping=10, line_mode=False, mode='train', test_csv=None, test_img=None, train_csv=None, train_img=None, validate_csv=None, validate_img=None)\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-f6f9ca4d51cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-38-f6f9ca4d51cc>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;31m# load training data, create TF model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoaderIAM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/CV1_train\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"/cv1_train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"/CV1_valid\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"/cv1_valid.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0mchar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/OELP_Word /dataloader_iam.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, train_img, train_csv, validate_img, validate_csv, batch_size, data_split)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mchars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# loading test images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path_train\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrain_csv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mimage_name_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mgt_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './dataset//cv1_train.csv'"]}],"source":["import argparse\n","import json\n","from typing import Tuple, List\n","\n","import cv2\n","import pandas as pd\n","import editdistance\n","from pathlib import Path\n","\n","from dataloader_iam import DataLoaderIAM, Batch\n","from model_phosc_cnn import Model, DecoderType\n","from preprocessor import Preprocessor\n","\n","\n","class FilePaths:\n","    \"\"\"Filenames and paths to data.\"\"\"\n","    fn_char_list = '/content/drive/MyDrive/OELP_Word /charList.txt'\n","    fn_summary = '/content/drive/MyDrive/OELP_Word /summary.json'\n","    fn_corpus = '/content/drive/MyDrive/OELP_Word /corpus.txt'\n","\n","\n","def get_img_height() -> int:\n","    \"\"\"Fixed height for NN.\"\"\"\n","    return 50\n","\n","\n","def get_img_size(line_mode: bool = False) -> Tuple[int, int]:\n","    \"\"\"Height is fixed for NN, width is set according to training mode (single words or text lines).\"\"\"\n","    return 250, get_img_height()\n","\n","\n","def write_summary(char_error_rates: List[float], word_accuracies: List[float]) -> None:\n","    \"\"\"Writes training summary file for NN.\"\"\"\n","    with open(FilePaths.fn_summary, 'w') as f:\n","        json.dump({'charErrorRates': char_error_rates, 'wordAccuracies': word_accuracies}, f)\n","\n","\n","def train(model: Model,\n","          loader: DataLoaderIAM,\n","          line_mode: bool,\n","          early_stopping: int = 25) -> None:\n","    \"\"\"Trains NN.\"\"\"\n","    epoch = 0  # number of training epochs since start\n","    summary_char_error_rates = []\n","    summary_word_accuracies = []\n","    preprocessor = Preprocessor(get_img_size(line_mode), data_augmentation=True, line_mode=line_mode)\n","    best_char_error_rate = float('inf')  # best valdiation character error rate\n","    no_improvement_since = 0  # number of epochs no improvement of character error rate occurred\n","    # stop training after this number of epochs without improvement\n","    while True:\n","        epoch += 1\n","        print('Epoch:', epoch)\n","\n","        # train\n","        print('Train NN')\n","        loader.train_set()\n","        while loader.has_next():\n","            iter_info = loader.get_iterator_info()\n","            batch = loader.get_next()\n","            batch = preprocessor.process_batch(batch)\n","            loss = model.train_batch(batch)\n","            print(f'Epoch: {epoch} Batch: {iter_info[0]}/{iter_info[1]} Loss: {loss}')\n","\n","        # validate\n","        char_error_rate, word_accuracy = validate(model, loader, line_mode)\n","\n","        # write summary\n","        summary_char_error_rates.append(char_error_rate)\n","        summary_word_accuracies.append(word_accuracy)\n","        write_summary(summary_char_error_rates, summary_word_accuracies)\n","\n","        # if best validation accuracy so far, save model parameters\n","        min_delta=0.0001\n","        if (char_error_rate + min_delta) <= best_char_error_rate:\n","            print('Character error rate improved, save model')\n","            best_char_error_rate = char_error_rate\n","            no_improvement_since = 0\n","            model.save()\n","        else:\n","            print(f'Character error rate not improved, best so far: {best_char_error_rate * 100.0}%')\n","            no_improvement_since += 1\n","\n","        # stop training if no more improvement in the last x epochs\n","        if no_improvement_since >= early_stopping:\n","            print(f'No more improvement since {early_stopping} epochs. Training stopped.')\n","            break\n","\n","\n","def validate(model: Model, loader: DataLoaderIAM, line_mode: bool) -> Tuple[float, float]:\n","    \"\"\"Validates NN.\"\"\"\n","    print('Validate NN')\n","    loader.validation_set()\n","    preprocessor = Preprocessor(get_img_size(line_mode), line_mode=line_mode)\n","    num_char_err = 0\n","    num_char_total = 0\n","    num_word_ok = 0\n","    num_word_total = 0\n","    while loader.has_next():\n","        iter_info = loader.get_iterator_info()\n","        print(f'Batch: {iter_info[0]} / {iter_info[1]}')\n","        batch = loader.get_next()\n","        batch = preprocessor.process_batch(batch)\n","        recognized, _ = model.infer_batch(batch)\n","\n","        print('Ground truth -> Recognized')\n","        for i in range(len(recognized)):\n","            num_word_ok += 1 if batch.gt_texts[i] == recognized[i] else 0\n","            num_word_total += 1\n","            dist = editdistance.eval(recognized[i], batch.gt_texts[i])\n","            num_char_err += dist\n","            num_char_total += len(batch.gt_texts[i])\n","            print('[OK]' if dist == 0 else '[ERR:%d]' % dist, '\"' + batch.gt_texts[i] + '\"', '->',\n","                  '\"' + recognized[i] + '\"')\n","\n","    # print validation result\n","    char_error_rate = num_char_err / num_char_total\n","    word_accuracy = num_word_ok / num_word_total\n","    print(f'Character error rate: {char_error_rate * 100.0}%. Word accuracy: {word_accuracy * 100.0}%.')\n","    return char_error_rate, word_accuracy\n","\n","\n","def infer(model: Model, fn_img: Path) -> None:\n","    \"\"\"Recognizes text in image provided by file path.\"\"\"\n","    img = cv2.imread(fn_img, cv2.IMREAD_GRAYSCALE)\n","    assert img is not None\n","\n","    preprocessor = Preprocessor(get_img_size(), dynamic_width=True, padding=16)\n","    img = preprocessor.process_img(img)\n","\n","    batch = Batch([img], None, 1)\n","    recognized, probability = model.infer_batch(batch, True)\n","    #print(f'Recognized: \"{recognized[0]}\"')\n","    #print(f'Probability: {probability[0]}')\n","    return recognized[0]\n","\n","def main():\n","    \"\"\"Main function.\"\"\"\n","\n","    parser = argparse.ArgumentParser()\n","    \n","    parser.add_argument('--mode', choices=['train','test'], default='train')\n","    parser.add_argument('--decoder', choices=['bestpath', 'beamsearch', 'wordbeamsearch'], default='bestpath')\n","    parser.add_argument('--batch_size', help='Batch size.', type=int, default=16)\n","    parser.add_argument('--test_img', help='Testing images ', required=False)\n","    parser.add_argument('--test_csv', help='Testing map ', required=False)\n","    parser.add_argument('--validate_img', help='Validation images ',  required=False)\n","    parser.add_argument('--validate_csv', help='Validation map ',  required=False)\n","    parser.add_argument('--train_img', help='Training images ',  required=False)\n","    parser.add_argument('--train_csv', help='Training map ', required=False)\n","    parser.add_argument('--line_mode', help='Train to read text lines instead of single words.', action='store_true')\n","    parser.add_argument('--early_stopping', help='Early stopping epochs.', type=int, default=10)\n","    parser.add_argument('--dump', help='Dump output of NN to CSV file(s).', action='store_true')\n","    args, unknown = parser.parse_known_args()\n","    # args = parser.parse_args()\n","    \n","    # set chosen CTC decoder\n","    decoder_mapping = {'bestpath': DecoderType.BestPath,\n","                       'beamsearch': DecoderType.BeamSearch,\n","                       'wordbeamsearch': DecoderType.WordBeamSearch}\n","    decoder_type = decoder_mapping[args.decoder]\n","\n","    # train or validate on IAM dataset\n","    if args.mode=='train':\n","        # load training data, create TF model\n","        print(args)\n","        loader = DataLoaderIAM(\"/CV1_train\",\"/cv1_train.csv\",\"/CV1_valid\",\"/cv1_valid.csv\",16)\n","        char_list = loader.char_list\n","\n","        # when in line mode, take care to have a whitespace in the char list\n","        if args.line_mode and ' ' not in char_list:\n","            char_list = [' '] + char_list\n","\n","        # save characters of model for inference mode\n","        open(FilePaths.fn_char_list, 'w').write(''.join(char_list))\n","\n","        # save words contained in dataset into file\n","        open(FilePaths.fn_corpus, 'w').write(' '.join(loader.train_words + loader.validation_words))\n","\n","        model = Model(char_list, decoder_type)\n","        train(model, loader, line_mode=args.line_mode, early_stopping=args.early_stopping)\n","\n","\n","    # test the saved model on a set of images\n","    elif args.mode == 'test':\n","        model = Model(list(open(FilePaths.fn_char_list).read()), decoder_type, must_restore=True, dump=args.dump)\n","        map_path = './' + args.test_csv\n","        img_path = './' + args.test_img\n","        #assert map_path.exists()\n","        #ssert img_path.exists()\n","        num_char_err = 0\n","        num_char_total = 0\n","        file = pd.read_csv(map_path)\n","        image_name_list = list(file['Image'])\n","        gt_list = list(file['Word'])\n","        total_words = 0\n","        words_ok = 0\n","        for i in range(len(image_name_list)):\n","            total_words = total_words + 1\n","            recognised_string = infer(model, img_path + image_name_list[i])\n","            if gt_list[i] == recognised_string:\n","                words_ok = words_ok + 1\n","            dist = editdistance.eval(recognised_string, gt_list[i])\n","            #print(\" recognised  and ground truth\",recognised_string, gt_list[i])\n","            num_char_err += dist\n","            num_char_total += len(gt_list[i])\n","            #print(\"Ground Truth : \",gt_list[i])\n","        print(\"Word Accuracy : \",round((words_ok/total_words)*100,2))\n","        print(\"Character Error rate : \",round((num_char_err/num_char_total)*100,2))\n","\n","if __name__ == '__main__':\n","    main()"]},{"cell_type":"code","source":[],"metadata":{"id":"nLMmbB8wUI3c"},"execution_count":null,"outputs":[]}]}