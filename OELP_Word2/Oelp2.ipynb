{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6476,"status":"ok","timestamp":1666607097843,"user":{"displayName":"Priyanshu Gupta","userId":"04165208456498233301"},"user_tz":-330},"id":"kOo22IY60CRX","outputId":"a768de8b-4c73-475f-edc6-f0197dcde384"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1666607097844,"user":{"displayName":"Priyanshu Gupta","userId":"04165208456498233301"},"user_tz":-330},"id":"sK-AnAOb0E8_"},"outputs":[],"source":["import sys\n","import os\n","sys.path.append(os.path.abspath(\"/content/drive/MyDrive/OELP_Word2\"))"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"executionInfo":{"elapsed":9095,"status":"ok","timestamp":1666607106929,"user":{"displayName":"Priyanshu Gupta","userId":"04165208456498233301"},"user_tz":-330},"id":"w4j3d0sijSo9","outputId":"42288f9e-668a-40ce-9d0e-7008bc9e837e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mon Oct 24 10:24:56 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   43C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]},{"output_type":"execute_result","data":{"text/plain":["'/device:GPU:0'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}],"source":["!nvidia-smi\n","import torch\n","torch.cuda.is_available()\n","import tensorflow as tf\n","tf.test.gpu_device_name()"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":486},"id":"7YTRP92r0IFX","outputId":"afc01ee0-a286-4f4b-c059-7ecf20ded893","executionInfo":{"status":"error","timestamp":1666607269822,"user_tz":-330,"elapsed":25,"user":{"displayName":"Priyanshu Gupta","userId":"04165208456498233301"}}},"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-be411313f3a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-7-be411313f3a6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;31m# load training data, create TF model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoaderIAM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_csv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_csv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0mchar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;31m# print(char_list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/OELP_Word2/dataloader_iam.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, train_img, train_csv, validate_img, validate_csv, batch_size, data_split)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mchars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# loading test images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path_train\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrain_csv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mimage_name_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mgt_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"NoneType\") to str"]}],"source":["import argparse\n","import json\n","from typing import Tuple, List\n","\n","import cv2\n","import pandas as pd\n","import editdistance\n","from pathlib import Path\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import numpy as np\n","\n","from dataloader_iam import DataLoaderIAM, Batch\n","from model_file import build_model, train_batch, infer_batch, get_Prediction_Model, CTCLoss\n","from preprocessor import Preprocessor\n","from Distiller import Distiller\n","\n","\n","class FilePaths:\n","    \"\"\"Filenames and paths to data.\"\"\"\n","    fn_char_list = '/content/drive/MyDrive/OELP_Word2/charList.txt'\n","    fn_summary = '/content/drive/MyDrive/OELP_Word2/summary.json'\n","    fn_corpus = '/content/drive/MyDrive/OELP_Word2/corpus.txt'\n","\n","\n","def get_img_height() -> int:\n","    \"\"\"Fixed height for NN.\"\"\"\n","    return 50\n","\n","\n","def get_img_size(line_mode: bool = False) -> Tuple[int, int]:\n","    \"\"\"Height is fixed for NN, width is set according to training mode (single words or text lines).\"\"\"\n","    return 250, get_img_height()\n","\n","\n","def write_summary(char_error_rates: List[float], word_accuracies: List[float]) -> None:\n","    \"\"\"Writes training summary file for NN.\"\"\"\n","    with open(FilePaths.fn_summary, 'w') as f:\n","        json.dump({'charErrorRates': char_error_rates, 'wordAccuracies': word_accuracies}, f)\n","\n","\n","def train(model,\n","          loader: DataLoaderIAM,\n","          line_mode: bool,\n","          early_stopping: int = 25) -> None:\n","    \"\"\"Trains NN.\"\"\"\n","    epoch = 0  # number of training epochs since start\n","    summary_char_error_rates = []\n","    summary_word_accuracies = []\n","    preprocessor = Preprocessor(get_img_size(line_mode), data_augmentation=True, line_mode=line_mode)\n","    best_char_error_rate = float('inf')  # best valdiation character error rate\n","    no_improvement_since = 0  # number of epochs no improvement of character error rate occurred\n","    # stop training after this number of epochs without improvement\n","    while True:\n","        epoch += 1\n","        print('Epoch:', epoch)\n","\n","        # train\n","        print('Train NN')\n","        loader.train_set()\n","        while loader.has_next():\n","            iter_info = loader.get_iterator_info()\n","            batch = loader.get_next()\n","            batch = preprocessor.process_batch(batch)\n","            loss = model.train_batch(batch)\n","            print(f'Epoch: {epoch} Batch: {iter_info[0]}/{iter_info[1]} Loss: {loss}')\n","\n","        # validate\n","        char_error_rate, word_accuracy = validate(model, loader, line_mode)\n","\n","        # write summary\n","        summary_char_error_rates.append(char_error_rate)\n","        summary_word_accuracies.append(word_accuracy)\n","        write_summary(summary_char_error_rates, summary_word_accuracies)\n","\n","        # if best validation accuracy so far, save model parameters\n","        min_delta=0.0001\n","        if (char_error_rate + min_delta) <= best_char_error_rate:\n","            print('Character error rate improved, save model')\n","            best_char_error_rate = char_error_rate\n","            no_improvement_since = 0\n","            model.save()\n","        else:\n","            print(f'Character error rate not improved, best so far: {best_char_error_rate * 100.0}%')\n","            no_improvement_since += 1\n","\n","        # stop training if no more improvement in the last x epochs\n","        if no_improvement_since >= early_stopping:\n","            print(f'No more improvement since {early_stopping} epochs. Training stopped.')\n","            break\n","\n","\n","def validate(model, loader: DataLoaderIAM, line_mode: bool) -> Tuple[float, float]:\n","    \"\"\"Validates NN.\"\"\"\n","    print('Validate NN')\n","    loader.validation_set()\n","    preprocessor = Preprocessor(get_img_size(line_mode), line_mode=line_mode)\n","    num_char_err = 0\n","    num_char_total = 0\n","    num_word_ok = 0\n","    num_word_total = 0\n","    while loader.has_next():\n","        iter_info = loader.get_iterator_info()\n","        print(f'Batch: {iter_info[0]} / {iter_info[1]}')\n","        batch = loader.get_next()\n","        batch = preprocessor.process_batch(batch)\n","        recognized, _ = model.infer_batch(batch)\n","\n","        print('Ground truth -> Recognized')\n","        for i in range(len(recognized)):\n","            num_word_ok += 1 if batch.gt_texts[i] == recognized[i] else 0\n","            num_word_total += 1\n","            dist = editdistance.eval(recognized[i], batch.gt_texts[i])\n","            num_char_err += dist\n","            num_char_total += len(batch.gt_texts[i])\n","            print('[OK]' if dist == 0 else '[ERR:%d]' % dist, '\"' + batch.gt_texts[i] + '\"', '->',\n","                  '\"' + recognized[i] + '\"')\n","\n","    # print validation result\n","    char_error_rate = num_char_err / num_char_total\n","    word_accuracy = num_word_ok / num_word_total\n","    print(f'Character error rate: {char_error_rate * 100.0}%. Word accuracy: {word_accuracy * 100.0}%.')\n","    return char_error_rate, word_accuracy\n","\n","\n","def infer(model, fn_img: Path) -> None:\n","    \"\"\"Recognizes text in image provided by file path.\"\"\"\n","    img = cv2.imread(fn_img, cv2.IMREAD_GRAYSCALE)\n","    assert img is not None\n","\n","    preprocessor = Preprocessor(get_img_size(), dynamic_width=True, padding=16)\n","    img = preprocessor.process_img(img)\n","\n","    batch = Batch([img], None, 1)\n","    recognized, probability = model.infer_batch(batch, True)\n","    #print(f'Recognized: \"{recognized[0]}\"')\n","    #print(f'Probability: {probability[0]}')\n","    return recognized[0]\n","\n","def main():\n","    \"\"\"Main function.\"\"\"\n","\n","    parser = argparse.ArgumentParser()\n","    \n","    parser.add_argument('--mode', choices=['train','test'], default='train')\n","    parser.add_argument('--decoder', choices=['bestpath', 'beamsearch', 'wordbeamsearch'], default='bestpath')\n","    parser.add_argument('--batch_size', help='Batch size.', type=int, default=16)\n","    parser.add_argument('--test_img', help='Testing images ', required=False)\n","    parser.add_argument('--test_csv', help='Testing map ', required=False)\n","    parser.add_argument('--validate_img', help='Validation images ',  required=False)\n","    parser.add_argument('--validate_csv', help='Validation map ',  required=False)\n","    parser.add_argument('--train_img', help='Training images ',  required=False)\n","    parser.add_argument('--train_csv', help='Training map ', required=False)\n","    parser.add_argument('--line_mode', help='Train to read text lines instead of single words.', action='store_true')\n","    parser.add_argument('--early_stopping', help='Early stopping epochs.', type=int, default=10)\n","    parser.add_argument('--dump', help='Dump output of NN to CSV file(s).', action='store_true')\n","    args, unknown = parser.parse_known_args()\n","    # args = parser.parse_args()\n","    \n","    \n","    # train or validate on IAM dataset\n","    if args.mode == 'train':\n","        # load training data, create TF model\n","        loader = DataLoaderIAM(args.train_img, args.train_csv, args.validate_img, args.validate_csv, args.batch_size)\n","        char_list = loader.char_list\n","        # print(char_list)\n","        # when in line mode, take care to have a whitespace in the char list\n","        if args.line_mode and ' ' not in char_list:\n","            char_list = [' '] + char_list\n","\n","        # save characters of model for inference mode\n","        open(FilePaths.fn_char_list, 'w').write(''.join(char_list))\n","\n","        # save words contained in dataset into file\n","        open(FilePaths.fn_corpus, 'w').write(' '.join(loader.train_words + loader.validation_words))\n","\n","        model = build_model(char_list)\n","        train(args.batch_size, model, char_list, loader, line_mode=args.line_mode, early_stopping=args.early_stopping)\n","\n","    # test the saved model on a set of images\n","    elif args.mode == 'test':\n","        char_list = list(open(FilePaths.fn_char_list).read())\n","        model = tf.keras.models.load_model('CTC-model_'+str(args.batch_size), custom_objects={'CTCLoss': CTCLoss})\n","        #model = get_Prediction_Model(model)\n","        #model=tf.keras.models.load_model('ravi_GW2_phos_64_'+\".h5\")\n","        map_path = './dataset/' + args.test_csv\n","        img_path = './dataset/' + args.test_img\n","        num_char_err = 0\n","        num_char_total = 0\n","        file = pd.read_csv(map_path)\n","        image_name_list = list(file['Image'])\n","        gt_list = list(file['Word'])\n","        total_words = 0\n","        words_ok = 0\n","        for i in range(len(image_name_list)):\n","            total_words = total_words + 1\n","            img = cv2.imread(img_path + image_name_list[i], cv2.IMREAD_GRAYSCALE)\n","            img = img.astype(np.float)\n","            img = cv2.transpose(img)\n","            img = img / 255\n","            recognised_string = infer_batch([img], model, char_list)\n","            #print(recognised_string, gt_list[i] )\n","            if gt_list[i] == recognised_string[0]:\n","                words_ok = words_ok + 1\n","            #print(gt_list[i] , recognised_string[0])\n","            dist = editdistance.eval(recognised_string[0], gt_list[i])\n","            num_char_err += dist\n","            num_char_total += len(gt_list[i])\n","        print(\"Word Accuracy : \", round((words_ok / total_words) * 100, 2))\n","        print(\"Character Error rate : \", round((num_char_err / num_char_total) * 100, 2))\n","\n","\n","if __name__ == '__main__':\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a5x3qFeeavK5","executionInfo":{"status":"aborted","timestamp":1666607107943,"user_tz":-330,"elapsed":15,"user":{"displayName":"Priyanshu Gupta","userId":"04165208456498233301"}}},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}